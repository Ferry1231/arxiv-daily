import os
import re
import json
import arxiv
import yaml
import logging
import argparse
import datetime
import requests
from typing import Dict, List

# é…ç½®æ—¥å¿—
logging.basicConfig(
    format='[%(asctime)s %(levelname)s] %(message)s',
    datefmt='%m/%d/%Y %H:%M:%S',
    level=logging.INFO
)

# å…¨å±€å¸¸é‡
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ARXIV_URL = "http://arxiv.org/"
GITHUB_API_URL = "https://api.github.com/search/repositories"

def load_config(config_file: str) -> Dict:
    # workspace = os.environ.get('GITHUB_WORKSPACE', os.getcwd())
    """åŠ è½½å¹¶å¤„ç†é…ç½®æ–‡ä»¶"""
    with open(config_file, 'r') as f:
        config = yaml.safe_load(f)
        
        # ä¸ºæ¯ä¸ªåˆ†ç±»ç”Ÿæˆå®Œæ•´è·¯å¾„
        for cat in config['categories'].values():
            # cat['output_dir'] = os.path.join(workspace, cat['output_dir'])
            cat['json_path'] = os.path.join(cat['output_dir'], 'papers.json')
            cat['md_path'] = os.path.join(cat['output_dir'], 'README.md')
            
        logging.info(f"Loaded configuration: {json.dumps(config, indent=2)}")
        return config

def get_authors(authors, first_author: bool = False) -> str:
    """æ ¼å¼åŒ–ä½œè€…åˆ—è¡¨"""
    return authors[0].name if first_author else ", ".join(a.name for a in authors)

def sort_papers(papers: Dict) -> Dict:
    """æŒ‰æ—¥æœŸæ’åºè®ºæ–‡"""
    return dict(sorted(papers.items(), key=lambda x: x[0], reverse=True))

def get_code_link(paper_id: str) -> str:
    """è·å–è®ºæ–‡ä»£ç é“¾æ¥"""
    try:
        code_url = f"https://arxiv.paperswithcode.com/api/v0/papers/{paper_id}"
        response = requests.get(code_url).json()
        return response.get('official', {}).get('url')
    except Exception as e:
        logging.error(f"Failed to get code link for {paper_id}: {str(e)}")
        return None

def fetch_papers(query: str, max_results: int) -> Dict:
    """è·å–æŒ‡å®šæŸ¥è¯¢çš„è®ºæ–‡"""
    client = Client()
    search = arxiv.Search(
        query=query,
        max_results=max_results,
        sort_by=arxiv.SortCriterion.SubmittedDate
    )
    
    papers = {}
    for result in client.results(search):
        paper_id = result.get_short_id()
        update_time = result.updated.date()
        
        # å¤„ç†ç‰ˆæœ¬å·
        clean_id = paper_id.split('v')[0]
        pdf_url = f"{ARXIV_URL}abs/{clean_id}"

        abstract = result.summary.replace("\n", " ")
        
        # è·å–ä»£ç é“¾æ¥
        code_link = get_code_link(clean_id) or "null"
        
        papers[paper_id] = {
            "title": result.title,
            "authors": get_authors(result.authors),
            "abstract": abstract,
            "pdf": pdf_url,
            "code": code_link,
            "updated": str(update_time)
        }
    
    print("paper.len: ", len(papers))
    
    # return papers

def update_category_data(category_config: Dict, global_config: Dict):
    """æ›´æ–°å•ä¸ªåˆ†ç±»çš„æ•°æ®"""
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(category_config['output_dir'], exist_ok=True)
    
    # è·å–è®ºæ–‡æ•°æ®
    papers = fetch_papers(
        query=category_config['query'],
        max_results=global_config['max_results']
    )
    
    # åŠ è½½ç°æœ‰æ•°æ®
    existing_data = {}
    if os.path.exists(category_config['json_path']):
        with open(category_config['json_path'], 'r') as f:
            existing_data = json.load(f)
    
    # åˆå¹¶æ–°æ•°æ®ï¼ˆå»é‡ï¼‰
    merged_data = {**papers, **existing_data}
    
    # ä¿å­˜æ›´æ–°åçš„æ•°æ®
    with open(category_config['json_path'], 'w') as f:
        json.dump(merged_data, f, indent=2)
    
    # ç”ŸæˆMarkdown
    generate_markdown(
        data=merged_data,
        output_path=category_config['md_path'],
        category_name=os.path.basename(category_config['output_dir'])
    )

def generate_markdown(data: Dict, output_path: str, category_name: str):
    """ç”Ÿæˆåˆ†ç±»ä¸“å±çš„Markdownæ–‡ä»¶"""
    # æ’åºè®ºæ–‡
    sorted_papers = sorted(data.values(), 
                         key=lambda x: x['updated'], 
                         reverse=True)
    
    # æ„å»ºMarkdownå†…å®¹
    content = [
        f"# {category_name.replace('-', ' ').title()} Papers",
        f"*Last Updated: {datetime.date.today().strftime('%Y-%m-%d')}*\n",
        "| Date | Title | Authors | PDF | Code |",
        "|------|-------|---------|-----|------|"
    ]
    
    for paper in sorted_papers:
        code_link = f"[Code]({paper['code']})" if paper['code'] != "null" else "null"
        row = f"| {paper['updated']} | {paper['title']} | {paper['authors']} | " \
              f"[PDF]({paper['pdf']}) | {code_link} |"
        content.append(row)
    
    # å†™å…¥æ–‡ä»¶
    with open(output_path, 'w') as f:
        f.write("\n".join(content))
    logging.info(f"Generated markdown for {category_name} at {output_path}")

def main(config: Dict):
    """ä¸»å¤„ç†æµç¨‹"""
    for cat_name, cat_config in config['categories'].items():
        logging.info(f"ğŸš€ Processing category: {cat_name}")
        try:
            update_category_data(cat_config, config)
            logging.info(f"Successfully updated {cat_name}")
        except Exception as e:
            logging.error(f"Failed to process {cat_name}: {str(e)}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='ArXivè®ºæ–‡è¿½è¸ªç³»ç»Ÿ')
    parser.add_argument('--config', type=str, default='config.yaml',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    args = parser.parse_args()
    
    config = load_config(args.config)
    main(config)